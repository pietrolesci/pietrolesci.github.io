---
layout: about
title: about
permalink: /
# subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Motto. Etc.

profile:
  align: right
  image: avatar.png
  image_circular: false # crops the image to make it circular
  # more_info: >
  #   <p>555 your office number</p>
  #   <p>123 your address street</p>
  #   <p>Your City, State 12345</p>

selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: false
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

I am a PhD student in Computer Science at the [University of Cambridge](https://www.cst.cam.ac.uk/) advised by Prof [Andreas Vlachos](https://andreasvlachos.github.io/). 
I have a background in economics and 3+ years experience across research labs, consulting firms, and international institutions training custom models and developing data science solutions. Find my cv here <a href="assets/pdf/pietro_lesci_cv.pdf"><i class="fas fa-file-pdf"></i></a>.

I am currently interested in understanding how training data influences a model’s behaviour.
To study this question, I draw methods from econometrics.
I aim to build models that can autonomously select and craft their own training data to continually acquire new capabilities as needed, thus blurring the line between data curation and learning.
My research intersects causal methods, active learning, tokenisation, and pre-training.
<!-- I am passionate about the science of language models: developing and applying causal methods---drawing from econometrics---to study the effect of training choices on models’ behaviour, including memorisation, shortcut learning, and tokenisation.  -->
<!-- I am currently interested in understanding how training data influences a model's behaviours, memorisation, and generalisation. The overarching goal of my work is to endow (language) models with the capabilities to choose (or craft) their training data autonomously to acquire new capabilities.  -->
<!-- I have a background in econometrics, from which I like drawing methods to study causal effects without interventional studies (e.g., retraining a model).  -->

My work has been presented at major machine learning conferences such as ICLR, ACL, NAACL, and EMNLP. 
I received a [Best Paper Award](https://2024.aclweb.org/program/best_papers/) at ACL 2024 and funding from the Translated Imminent [Research Grant](https://imminent.translated.com/the-5-innovative-projects-awarded-with-imminent-research-grants-2023) for my research contributions. 
You can find the complete list of publications and related links on the [/research](research) page. I also keep my [<img src="assets/img/huggingface_logo-noborder.svg" alt="Hugging Face" style="height: 1em; vertical-align: middle;"> profile](https://huggingface.co/pietrolesci) updated: each paper is linked to a collection listing all the relevant artefacts.






