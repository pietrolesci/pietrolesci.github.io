---
layout: distill
title: A gentle introduction to bayesian nonparametrics
description: Notes wrtten while studying for my MSc at Bocconi University
tags: bayesian-nonparametrics
giscus_comments: true
date: 2019-01-15
featured: false
map: true

authors:
  - name: Pietro Lesci
    url: "https://pietrolesci.github.io/"
    affiliations:
      name: Bocconi University

bibliography: 2019-01-15-intro-bnp.bib

toc: true
---

# Introduction

In this chapeter we will provide the core theoretical ideas behind the nonparametric approach and how it is implemented from a Bayesian perspective. We will review basic definitions and set-out the notation that will be used in the book. We will describe how the challenging task of constructing priors on infinite-dimensional objects has been tackeld.

Bayesian nonparametrics concerns _Bayesian inference_ methods for _nonparametric_ models. A nonparametric model involves at least one infinite-dimensional parameter and hence may also be referred to as an "infinite-dimensional model". Indeed, the nomenclature "nonparametric" is misleading since it gives the impression that there are no parameters in the model, while in reality there are infinitely many unknown quantities. Examples of infinite-dimensional parameter are functions or measures. The basic idea of nonparametric inference is to use data to infer an unknown quantity while making as few assumptions as possible. Usually, this means using statistical models that are infinite-dimensional.

Where does Bayesian nonparametrics fit? We can distinguish four sectors of statistical methodologies, defined by the interplay two factors <d-cite key="walker2010"></d-cite> whose "cartesian product" defines four "boxes":

* Parametric vs Nonparametric
* Frequentist vs Bayesian

The _Frequentist parametrics_ box contains methods, developed from 1920s onwards, like maximum likelihood, optimum tests, with calculation of p-values, optimal estimators, confidence intervals, and so forth. Some of the procedures stem from exact probability calculations while others (a lot of them) relate to the application of large-sample techniques.

The _Bayesian parametrics_ box comprises classic methodology for prior and posterior distributions in models with a finite number of parameters. Such methods, starting from the premise that uncertainty about model parameters may somehow be represented in terms of probability distributions, have been around for more than a hundred years (the Bayes theorem dates back to the 1763), but they were limited to a to simple statistical models and priors. The applicability of Bayesian parametrics widened significantly with the advent of modern computers and the development of further numerical methods and software packages pertaining to numerical integration and Markov chain Monte Carlo (MCMC) simulations. Asymptotics is also useful for Bayesian parametrics.

The _Frequentist nonparametrics_ box contains mixed objects. The term has historically been associated with test procedures that are "distribution free" leading also to nonparametric confidence intervals and bands. Furthermore, still belong to this box methodologies for the estimation of probability densities and regression functions without parametric assumptions; and also specific computational techniques such as the bootstrap. 

Finally, the _Bayesian nonparametrics_ box comprises models and methods characterized by big (infinite) parameter spaces and construction of probability measures over these spaces. Typical examples include Bayesian setups for density estimation, nonparametric regression with or without a fixed error distribution, survival function estimation for survival analysis, etc. In the Bayesian nonparametric paradigm, a prior distribution is assigned to all relevant unknown quantities, whether finite or infinite dimensional. The posterior distribution is the conditional distribution of these quantities, given the data, and is the basis for all inference -- as in any Bayesian inference, except that the unknown quantities or parameters may be infinite dimensional in this case. A model completely specifies the conditional distribution of all observed, given all unobserved quantities, while a prior distribution specifies the distribution of all unobservables. The posterior distribution involves an inversion of the order of conditioning and gives the distribution of the unobservables given the observables. Latent variables are unobservables and are treated in the same way as the unknown parameters used to describe the model.

Why adopt the nonparametric Bayesian approach for inference? Nonparametric models can allow one to avoid the arbitrary and possibly unverifiable assumptions inherent in parametric models. While Bayesian procedures may be desirable for philosophical or practical reasons.

Let's now look at the matter from another point of view: let's try to interpret Bayesian parametric models from a Bayesian nonparametric standpoint. Parametric models make restrictive assumptions about the data generating mechanism, which may cause serious bias in inference. In the Bayesian framework a parametric model assumption can be viewed as an extremely strong prior opinion. Indeed, a parametric model specification $X\vert\theta \sim p_\theta$, for $\theta\in\Theta\subset\mathbb{R}^d$, with a prior $\theta\sim\Pi$, may be considered within a nonparametric Bayesian framework as $X\vert p \sim p$, for $p\in\mathcal{P}$ with $\mathcal{P}$ a set of densities equipped with a prior $p \sim \Pi$ with the property that $\Pi\left(\{p_\theta : \theta\in\Theta\}\right) = 1$. Thus parametric modelling is equivalent to insisting on a prior that assigns probability one to a thin subset of all densities.


## The problem

A Bayesian analysis cannot proceed without a prior distribution on all parameters. A prior ideally expresses a quantification of pre-experiment and subjective knowledge. A prior on a function requires knowing many aspects of the function -- e.g. for a probability density function that it integrates to one -- including the ability to quantify the information in the form of a _probability measure_.

This poses an apparent conceptual contradiction, as expressed in @ghosal2017: "A nonparametric Bayesian approach is pursued to minimize restrictive parametric assumptions, but at the same time requires specification of the minute details of a prior on an infinite-dimensional parameter}. 

It is usually thought that inference must be based on an _objective prior_. This is vaguely understood as a prior that is proposed by some automatic mechanism that is not in favour of any particular parameter values, and has low information content compared to the data -- also called _default priors_.  Some of the earliest statistical analyses in this way suggested to use a uniform prior. Uniform priors were strongly criticised for lacking invariance (e.g. to nonlinear trasformations). However, invariance-friendly methods such as Jeffreys' priors and reference analysis arised, although most of these ideas are restricted to finite-dimensional parametric problems. An objective prior should be automatically constructed using a default mechanism. It need not be non-informative, but should be spread all over the parameter space <d-cite key="ghosal2017"></d-cite>. Unlike in parametric situations, where non-informative priors are often _improper_, default priors considered in nonparametric Bayesian inference are almost always  _proper_. Large support of the prior means that the prior is not too concentrated in some particular region. This generally causes that the prior is subdued gradually by the data if the sample size increases, so that eventually the data overcome the prior. 

A prior should also have some "good" properties such as __robustness__: Bayesian robustness means that the choice of the prior does not influence the posterior distribution too much. Another way to formulate the problem is to study the _asymptotic properties_ of the prior, as the information in the data increases indefinitely, such as _posterior consistency_ -- which means that the posterior probability eventually concentraes in a (any) small neighborhood of the actual value of the parameter -- and _rate of convergence_ or functional limit of the prior.

Another "good" property a prior must have is a "nice" structure. This is important in the context of computations: we cannot directly simulate from the posterior distribution with infinitely many parameters in a finite time. Therefore, unless it is parameterized by finitely many parameters the problem is infeasible. We must break up the function of interest into more elementary finite-dimensional objects, and simulate from their posterior distribution. For this reason the structure of the prior is important. Useful, or nice, structures may come from conjugacy or approximation. Loosely speaking, what we usually try to do is to integrate out the infinite-dimensional parameter given the latent variables. 


## Intermezzo: basic concepts

 __Probability space.__ A _probability space_ is a triple $\left(\Omega, {\mathcal{A}}, P\right)$ consisting of:

* The _sample space_ $\Omega$: an arbitrary non-empty set

* The $\sigma$-_algebra_ $\mathcal{A} \subseteq 2^\Omega$ (also called $\sigma$-field): a set of subsets of $\Omega$, called events, such that:
    
    * It contains the sample space, $\Omega \in \mathcal{A}$
    
    * It is closed under complements: if $A\in\mathcal{A}$, then also $(\Omega \setminus A)\in \mathcal{A}$
    
    * It is closed under countable unions: if $A_i\in \mathcal{A}$ for $i=1,2,\dots$, then also $(\bigcup_{i=1}^\infty A_i)\in \mathcal{A}$; by the De Morgan's law it also holds that $\mathcal{A}$ is also closed under countable intersections: if $A_{i}\in\mathcal{A}$ for $i=1,2,\dots$, then also $(\bigcap_{i=1}^\infty A_i) \in \mathcal{A}$

* The _probability measure_ $P:\mathcal{A}\to [0,1]$: a function on $\mathcal{A}$ such that: 
    
    * It is countably additive (also called σ-additive): 
  $$\text{if } \{ A_i \}_{i=1}^{\infty} \subseteq \mathcal{A} \text{ is a countable collection of pairwise disjoint sets, then } P(\bigcup_{i=1}^\infty A_{i}) = \sum_{i=1}^\infty P(A_i)$$
    
    * The measure of entire sample space is equal to one: $P(\Omega)=1$

__Measure.__ Let $\Omega$ be a set and $\mathcal{A}$ a $\sigma$-algebra over $\Omega$. A function $\mu: \Omega \to \mathbb{R}$, the extended real line, is called a _measure_ if it satisfies the following properties<d-footnote>Note that the greatest difference with a probability measure $P$ is that $\mu(\Omega)$ is not bounded in $[0,1]$, that is they differ in their support.</d-footnote>:

* _Non-negativity_: For all $A$ in $\mathcal{A}: \mu(A) \geq 0$

* _Null empty set_: $\mu(\varnothing)=0$.

* _Countable additivity (or σ-additivity)_:
$$\text{For all countable collections } \{A_i\}_{i=1}^\infty \text{ of pairwise disjoint sets in } \Omega: \mu \left( \bigcup_{k=1}^\infty A_k \right) = \sum _{k=1}^\infty \mu(A_k)$$

__Measurable space.__ A measurable space is a pair $(\Omega, \mathcal{A})$ consisting of a set $\Omega$ and a $\sigma$-algebra $\mathcal{A}$ of subsets of $\Omega$. Also, $A\in\mathcal{A}$ are called _measurable sets_.

__Measurable function.__ Let $(X, \mathcal{A})$ and $(Y, \mathcal{B})$ be measurable spaces. A map $f: X \to Y$ is called measurable if $f^{-1}(B) \in \mathcal{A}$ for every $B \in \mathcal{B}$.

__Random variable.__ If $P$ is a probability measure on $(\Omega, \mathcal{A})$, a random variable $X$ taking values in $\mathrm{X}$ is simply a measurable function $X : \Omega \to \mathrm{X}$. Intuition: think of the probability space $(\Omega, \mathcal{A}, P)$ as a black-box random number generator, and $X$ as a function taking random samples in $\Omega$ and producing random samples in $\mathrm{X}$. 

__Markov Kernel.__ Let $(X, \mathcal{A}), (Y, \mathcal {B})$ be measurable spaces. A Markov kernel with source $(X, \mathcal{A})$ and target $(Y, \mathcal{B})$ is a map $\kappa: X \times \mathcal{B} \to [0,1]$ with the following properties:

* The map $x \mapsto \kappa(x,B)$ is $\mathcal{A}$-measurable for every $B \in \mathcal{B}$

* The map $B \mapsto \kappa(x,B)$ is a probability measure on $(Y, \mathcal{B})$ for every $x \in X$

In other words it associates to each point $x \in X$ a probability measure $\kappa(x,\cdot)$ on $(Y, \mathcal{B})$ such that, for every measurable set $B \in \mathcal{B}$, the map $x \mapsto \kappa(x,B)$ is measurable with respect to the $\sigma$-algebra $\mathcal{A}$.

__Polish space.__ A topological space is called Polish if its topology is generated by a metric that makes it complete and separable. A metric space $\mathbb{M}$ is called _complete_ if every Cauchy sequence is convergent. A topological space is called _separable_ if there exists a sequence $\\{ x_n \\}_{n=1}^{\infty}$ of elements of the space such that every nonempty open subset of the space contains at least one element of the sequence.

__Borel set.__ A Borel set is any set in a topological space $X$ that can be formed from open sets through the operations of countable union, countable intersection, and relative complement. The collection of all Borel sets on $X$ forms a $\sigma$-algebra, known as the Borel $\sigma$-algebra, which is the smallest $\sigma$-algebra containing all open sets.

__Stochastic process.__ A stochastic process indexed by a set $\mathbb{I}$ is a collection $W = \left(W(i): i \in \mathbb{I}\right)$ of random variables defined on a probability space. A sample path $i \rightarrow W(i)$, given an $i\in\mathbb{I}$, of $W$ is a _random function_ and hence the law of $W$, the _law of the set of sample paths_, is a prior on a space of functions $f: \mathbb{I} \rightarrow \mathbb{R}$.

__Regular conditional probability.__<d-footnote>Motivation: Normally we define the conditional probability of an event $A$ given an event $B$ as: $P(A\vert B)= \frac{P(A\cap B)}{P(B)}$. The difficulty with this arises when the event $B$ is too small to have a non-zero probability.</d-footnote> Let $(\Omega, \mathcal{A}, P)$ be a probability space, and let $X:\Omega \rightarrow \mathrm{X}$ be a random variable, defined as a Borel-measurable function from $\Omega$ to its state space $(\mathrm{X}, \mathcal{B})$. Then a regular conditional probability is defined as a function $\nu: \mathrm{X} \times \mathcal{A} \rightarrow [0,1]$, i.e. a Markov kernel, such that for all $A \in \mathcal{A}$ and all $B \in \mathcal{B}$, $P \left( A \cap X^{-1}(B) \right) = \int_{B} \nu(x,A) \; P \left(X^{-1}(dx)\right)$, or equivalently, $P(A\vert X=x) = \nu(x,A)$.



## Priors on spaces of probability measures: Definition

Now that we have review some basic definitions, we can proceed analysing the construction of priors properly. Let's repeat, once, that in the Bayesian framework the data $X$ follows a distribution determined by a parameter $\theta$, which is itself considered to be generated from a prior distribution $\Pi$. The corresponding posterior distribution is the conditional distribution of $\theta$ given $X$. This framework is identical in parametric and nonparametric Bayesian statistics, the only difference being the dimensionality of the parameter. Obviously, proper definitions of priors and (conditional) distributions require more care in the nonparametric case. Let's formalize the problem.

In the nonparametric setting it is natural to placing a prior distribution directly on the model. However, this is complicated. To make the task easier, usually it is assumed that the sample space $(\mathrm{X}, \mathcal{A})$ is a Polish space with its Borel $\sigma$-field (the smallest $\sigma$-field containing all open sets). This ensures some nice mathematical properties. The prior is defined on the collection $\mathrm{M} = \mathrm{M}(\mathrm{X})$ of all probability measures on $(\mathrm{X}, \mathcal{A})$ are consided. Simmetrically, we could define a prior on $\mathrm{M} = \mathrm{M}(\Theta)$ the set of all probability measures on $(\Theta, \mathcal{B})$ the parameter space.

A prior $\Pi$ on $\mathrm{M}$ is a probabiity measure on a $\sigma$-field of subsets of $\mathrm{M}$. We can look at this prior from another point of view though, perhaps more intuitive. A prior on $\mathrm{M}$ can be viewed as the _law of a random measure_ $P$ and can be identified with the collection of "random probabilities} $P(A)$ of sets $A \in \mathcal{A}$. Usually the measurability structure of $\mathrm{M}$ is choosen such that each of these $P(A)$ is a random variable, and thus $\left(P(A): A \in \mathcal{A}\right)$ is a stochastic process on the underlying probability space: given a specific $A$, $P(A)$ is random because $P$ is random and it is distributed according to this prior we want to assign. It is a strange (complicated) stochastic process: the index set is composed by the elements of the $\sigma$-algebra $\mathcal{A}$!

Define $\mathcal{M}$ the smallest $\sigma$-field that makes all maps in $\mathrm{M}$, $P: \mathcal{A} \to \mathbb{R}$, measurable for $A \in \mathcal{A}$. Thus, the priors $\Pi$'s are measures on $(\mathrm{M}, \mathcal{M})$. Now we can restate the problem of finding a prior on all probability measures defined on $\mathrm{X}$ more clearly. 

The parameter $\theta$ that indexes the statistical model $(P_\theta: \theta\in\Theta)$ can be taken equal to the distribution $P$ itself, with $\mathrm{M}$ as the parameter set, giving a model of the form $(P : P \in\mathrm{M})$. In this way, the $P$'s are Markov kernels from $(\mathrm{M}, \mathcal{M})$ to $(\mathrm{X}, \mathcal{A})$. 

Let's recap what we have stated so far. A statistical model on a sample space $\mathrm{X}$ is a subset of probability measures, $M \subset \mathrm{M}(\mathrm{X})$, on $\mathrm{X}$. The elements of the subset $M$ are indexed by a parameter $\theta$ with values in a parameter space $\Theta$, that is,
$$M = \\{P_\theta \vert \theta \in \Theta\\}$$
We call a model parametric if $\Theta$ has finite dimension, which usually means $\Theta \subset \mathbb{R}^d$ for some $d \in \mathbb{N}$. If $\Theta$ has infinite dimension, _M_ is called a nonparametric model: there is an infinite number of probability measures to choose from. Therefore, defining a nonparametric Bayesian model means defining a prior distribution on an infinite-dimensional space.


### Example

As usual we formulate a statistical problem assuming that $n$ observations $x_1,\dots, x_n$ with values in $\mathrm{X}$ are collected. We model them as random variables $X_1,\dots, X_n$. In classical statistics, we assume that these random variables are generated i.i.d. from a measure, $P_\theta$, in the model, $M$, that is

$$X_1,\dots, X_n \stackrel{\small i.i.d.}{\sim} P_\theta, \qquad \theta \in \Theta$$

The objective of statistical inference is then to draw conclusions about the value of $\theta$ and, hence, about the distribution $P_\theta$ -- which is indexed by $\theta$ -- from the observations.

In Bayesian statistics, we model the parameter as a random variables: a basic principle of Bayesian statistics is that all forms of uncertainty should be expressed as randomness. We therefore have to interpret $\theta$ as a random variable with values in $\Theta$. Thus, the parameter set $\Theta$ is equipped with a $\sigma$-field $\mathcal{B}$, thus $(\Theta, \mathcal{B})$ is a measurable space. The prior, $\Pi: \mathcal{B} \to [0,1]$, is a probability measure on this measurable space; thus $(\Theta, \mathcal{B}, \Pi)$ is a probability space. 

Then, we assume that the distribution $P_\theta$ of $X$ given $\theta$ is a Markov kernel from $(\mathrm{X}, \mathcal{A})$ to $(\Theta, \mathcal{B})$, that is $P_\theta: \mathrm{X} \times \mathcal{B} \to [0, 1]$, i.e. a _regular conditional distribution_ on the measurable space $(\mathrm{X}, \mathcal{A})$ such that $P_\theta(A)$ is a probability measure for every $\theta \in \Theta$ and is measurable for every $A \in \mathcal{A}$.

Then the pair $(X, \theta)$ has a well defined joint distribution on the product space $(\mathrm{X}\times\Theta, \mathcal{A}\times\mathcal{B})$, given by
$$\mathrm{Pr}(X \in A, \theta \in B) = \int_B P_\theta(A)d\Pi(\theta)$$
This gives rise to the marginal distribution of $X$, defined by
$$\mathrm{Pr}(X \in A) = \int P_\theta(A) d\Pi(\theta)$$
By Kolmogorov's definition of conditional probabilities, $\mathrm{Pr}(\theta\in B \vert X)$ for $B\in\mathcal{B}$ is always well defined, as measurable function of $X$.

The task to compute the __posterior distribution__ is then equivalent to the task of finding a __Markov kernel__ from $(\Theta, \mathcal{B})$ to $(\mathrm{X}, \mathcal{A})$, that is $P_X: \Theta \times \mathcal{A} \to [0,1]$, i.e. a _regular conditional distribution_ on the measurable space $(\Theta, \mathcal{B})$. A sufficient condition for the existence of such Markov kernel is that $\Theta$ is a _Polish space_ and $\mathcal{B}$ its Borel $\sigma$-algebra.

Hence, the model $P_\theta$, from a Bayesian perspective, consists of a model $M = \\{P_\theta: \theta\in\Theta\\}$ as above, called the observation model (or likelihood), and a prior $\Pi$. The data are, thus, generated in two stages (capital letters are used to define random variables)

$$
  \theta \sim \Pi X_1,X_2, \dots \vert \theta \stackrel{\small i.i.d.}{\sim} P_\theta
$$

The data are _conditionally_ i.i.d. rather than i.i.d. Our objective is then to determine the posterior distribution, i.e. the conditional distribution of $\theta$ given the data

$$
  \Pi\left(\theta \in B \vert X_1 = x_1,\dots, X_n = x_n\right)
$$

The value of the parameter remains uncertain given a finite number of observations, and Bayesian statistics uses the posterior distribution to express this uncertainty. 

Now we have identified what we are looking for, but how do we practically construct such priors on the space of all probability measures?


## Priors on spaces of probability measures: Construction

The exist few methods to construct priors on spaces of probability measure, we will only review the most known (and simple).


### Construction through a stochastic process

The best way to familiarize with this approach is to think that a distribution on an infinite-dimensional space $\Theta$ is a stochastic process with paths in $\Theta$ <d-cite key="orbanz2014"></d-cite>. Using random processes, we are merely constructing random density functions with unrestricted shapes. The prior, in this case, becomes the law governing the stochastic process.

One general method of constructing a random measure is to start with the stochastic process $(P(A): A \in \mathcal{A})$, constructed using Kolmogorov's consistency theorem: this theorem guarantees that a suitably "consistent} collection of finite-dimensional distributions will define a stochastic process. Next step is to show that this process can be realized within $\mathrm{M}$, viewed as a subset of $R^{\mathcal{A}}$, the space of all functions that from $\mathcal{A}$ go in $\mathbb{R}$. 

The details are as follows. For every _finite_ collection $A_1, \dots, A_k$ of Borel sets in $\mathrm{X}$, the vector $(P(A_1), \dots, P(A_k))$ of probabilities obtained from a random measure $P$ is an ordinary random vector in $\mathbb{R}^k$ -- given $P$, it becomes simply a vector of numbers between 0 and 1. The construction of $P$ may start with the specification of the distributions of all vectors of this type. For any consistent specification of the distributions, Kolmogorov’s theorem allows us to construct, on a suitable probability space $(\Omega, \mathcal{A}, P)$, a stochastic process $(P(A): A \in \mathcal{A})$ with the given finite-dimensional distributions.

__Example.__ A simple and important example is to specify the distributions of each one of these vectors as Dirichlet distributions with parameter vector $(\mu(A_1), \dots, \mu(A_k))$, for a given Borel measure $\mu$ -- a Borel measure is any measure $\mu$ defined on a Borel $\sigma$-algebra. Intuitively $\mu$ returns the probability of the partition $A_k$. 


### Construction in Countable Sample Spaces
A probability distribution on a countable sample space, say, $\Theta$, equipped with the $\sigma$-field $\mathcal{B}$, can be represented as an infinite-length probability vector $s = (s_1, s_2, \dots)$, where each $s_k$ gives the probability of a partition $B_k$ in the $\sigma$-field. Basically we are saying that each vector $s$ is a probability measure on $\Theta$. As any probability measure, we would like each component of the vector to be positive and the infinite sum of the components be equal to one.

The set $\mathrm{M}$ in this case is the space of all these vectors $s$. A prior on $\mathrm{M}$ can therefore be identified with the distribution of a random element, a vector $s_n$, with values in the countable-dimensional unit __simplex__ (that can be imagined as a multidimensional triangle), and we denote it as

$$
\Delta := \left\{(s_n)_{n\in\mathbb{N}}\colon s_n \geq 0 \quad \text{and} \quad \sum_{n=1}^{\infty} s_n = 1\right\}
$$

That is, it is the space of sequences $s_n$ that respect the two properties stated: each element greater than zero and the sum of the elements equal to 1. We write $\mathrm{M} \stackrel{\small def}{=} \Delta$. The $\sigma$-field $\mathcal{M}$ on $\Delta$ is assumed to be generated by the coordinate map $i \mapsto s_i$ for $i\in\mathbb{N}$. 

What we want to assign a probability measure to each of one of these sequences, that is we want to assign our prior. 

A map $\pi$ from some probability space into $\Delta$ is a random element if and only if every coordinate variable $p_i$ is a random variable. Hence a prior simply corresponds to an infinite sequence of nonnegative random variables $\pi_1, \pi_2, \dots$ that adds up to 1. Constructing priors using Kolmogorov's theorem applies, but can be simplified by ordering the coordinates: it suffices to construct consistent marginal distributions for $(\pi_1,\dots, \pi_k)$, for every $k = 1, 2, \dots \in \mathbb{N}$. The way we construct these consistent marginal distribution is by __normalization__ or __stick-breaking__.


#### Construction through normalization
Given nonnegative random variables $Y_1, Y_2, \dots$ such that $\sum_{i=1}^{\infty} Y_i$ is positive and converges _almost surely_, we can define a prior on $\Delta$ by putting

$$
  \pi_k = \frac{Y_k}{\sum_{i=1}^{\infty} Y_i}, \qquad k\in\mathbb{N}
$$

A simple, sufficient condition for the convergence of the random series is that $\sum_{i=1}^{\infty} \mathrm{E}(Y_i) < \infty$. Usually, for convenience, $Y_i$ are assumed independent.


#### Construction through Stick-Breaking

Stick-breaking is a technique to construct a prior directly on $\Delta$ <d-cite key="walker2010"></d-cite> @ghosal2017]. The problem at hand is to distribute the total mass 1, which we identify with a stick of length 1, randomly to each element of $\mathbb{N}$. We first break the stick at a point given by the realization of a random variable $0 \leq V_1 \leq 1$ and assign mass $V_1$ to the point $1 \in \mathbb{N}$. Of course we have to choose to sample the $V$'s from a suitable distribution: usually the $\mathrm{Beta}(a_1, b_1)$ is used. We think of the remaining mass $1 - V_1$ as a new stick, and break it into two pieces of relative lengths $V_2$ and $1-V_2$ according to the realized value of another random variable $0 \leq V_2 \leq 1$. We assign mass $(1 - V_1)V_2$ to the point $2 \in \mathbb{N}$, and are left with a new stick of length $(1 - V_1)(1 - V_2)$. Continuing in this way, we assign mass to the point $k\in\mathbb{N}$ equal to

$$
  \pi_k = \left(\prod_{i=1}^{k-1}(1- V_i) \right) V_k
$$

Clearly, by continuing to infinity, this scheme will attach a random subprobability distribution to $\mathbb{N}$ for any sequence of random variables $V_1, V_2, \dots$ with values in $[0, 1]$. Under mild conditions, the probabilities $\pi_k$ will sum to one.


### Construction through a Randomly Selected Discrete Set

A complementary approach is to construct priors through __structural definitions__, that is collect priors on measures on a general Polish space that are defined explicitly from mathematical theory. On example is the construction of a prior using randomly selected discrete sets in $\mathbb{N}$ <d-cite key="ghosal2017"></d-cite>.

Given an integer $k \in \mathbb{N} \cup \\{\infty\\}$, nonnegative random variables $\pi_{1},\dots, \pi_{k}$ with $\sum_{i=1}^k \pi_{i} = 1$ and random variables $\theta_{1},\dots, \theta_{k}$ taking their values in $(\Theta, \mathcal{B})$, we can define a random probability measure by 

$$
  P = \sum_{i=1}^{k} \pi_{i} \delta_{\theta_{k}}
$$

The realizations of this prior are discrete with finitely or countably many support points, which may be different for each realization. Given the number, $n$, of support points, their "weights} $\pi_{1},\dots, \pi_{k}$ and their "locations} $\theta_{1},\dots, \theta_{k}$ are often chosen independent. An important special case is obtained by choosing $k = \infty$, yielding a prior of the form

$$
  P = \sum_{i=1}^{\infty} \pi_{i} \delta_{\theta_{i}}
$$

Further specializations are to choose $\theta_1, \theta_2, \dots$ an i.i.d. sequence in $\Theta$, and to choose the weights $\pi_1, \pi_2, \dots$ independently by the stick-breaking algorithm.

There are, of course, other ways to construct such priors. We stop here to explore in more detail how these methods are actually implemented in applied cases. In the next chapter we will explore a very common example of a prior on the space of probability measures: the _Dirichlet Process_ starting with a very important use-case: density estimation.