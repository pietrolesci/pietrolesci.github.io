<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> research | Pietro Lesci </title> <meta name="author" content="Pietro Lesci"> <meta name="description" content="Pietro Lesci's personal website "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%87%92&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pietrolesci.github.io/research/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Pietro</span> Lesci </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/outreach/">outreach </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">research</h1> <p class="post-description"></p> </header> <article> <p>I am passionate about the science of language models: developing methods—also drawing from econometrics—to study the effect of training data on models’ behaviour. Currently, I focus on active learning, data valuation, and memorisation estimation. See the up-to-date list of publications on my <a href="https://scholar.google.com/citations?user=uRIcVlAAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank">Google Scholar</a> page.</p> <p><span class="mystar">*</span> denotes equal contribution.</p> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h1>Conference &amp; Journal Articles</h1> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL 2025</abbr> </div> <div id="lesci-etal-2025-causal" class="col-sm-8"> <div class="title">Causal Estimation of Tokenisation Bias</div> <div class="author"> <em>Pietro Lesci</em>, Clara Meister, Thomas Hofmann, Andreas Vlachos, and Tiago Pimentel </div> <div class="periodical"> <em>In (To Appear) Proceedings of the 63nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2506.03149" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="arxiv-logo" src="/assets/img/arxiv-logo.svg" alt="arXiv" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/arxiv-logo-light.svg' : '/assets/img/arxiv-logo.svg'"> </a> <a href="https://github.com/pietrolesci/tokenisation-bias" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="github-logo" src="/assets/img/github-mark.svg" alt="GitHub" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/github-mark-light.svg' : '/assets/img/github-mark.svg'"> </a> <a href="https://huggingface.co/collections/pietrolesci/tokenisation-bias-66d5d0b40cb82a2d789b19db" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/huggingface_logo-noborder.svg" alt="GitHub" style="height: 14px; vertical-align: middle;"> </a> </div> <div class="abstract hidden"> <p>Modern language models are typically trained over subword sequences, but ultimately define probabilities over character-strings. Ideally, the choice of the tokeniser—which maps character-strings to subwords—should not affect the probability assigned to the underlying character-string; in practice, it does. We define this mismatch as tokenisation bias. In this work, we quantify one particular type of tokenisation bias: the effect of including or not a subword (e.g., ⟨hello⟩) in a tokeniser’s vocabulary on the probability a trained model assigns to the corresponding characters (i.e., "hello"). Estimating this effect is challenging because each model is trained with only one tokeniser. We address this by framing tokenisation bias as a causal effect and estimating it using the regression discontinuity design. Specifically, we exploit the fact that tokenisation algorithms rank subwords and add the first K to a tokeniser’s vocabulary, where K is an arbitrary cutoff point. As such, we can estimate a causal effect by comparing similar subwords around this cutoff. Experimentally, we find that tokenisation consistently affects models’ outputs across scales, vocabularies, and tokenisers. Notably, a subword’s presence in a small model’s vocabulary may increase its characters’ probability by up to 17 times, highlighting tokenisation as a key design choice in language modelling.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NAACL 2025</abbr> </div> <div id="luo-etal-2024-large" class="col-sm-8"> <div class="title">Self-Training Large Language Models for Tool-Use Without Demonstrations</div> <div class="author"> Ne Luo, Aryo Pradipta Gema, Xuanli He, Emile Krieken, <em>Pietro Lesci</em>, and Pasquale Minervini </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: NAACL 2025</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2502.05867" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="arxiv-logo" src="/assets/img/arxiv-logo.svg" alt="arXiv" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/arxiv-logo-light.svg' : '/assets/img/arxiv-logo.svg'"> </a> <a href="https://github.com/neneluo/llm-tool-use" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="github-logo" src="/assets/img/github-mark.svg" alt="GitHub" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/github-mark-light.svg' : '/assets/img/github-mark.svg'"> </a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) remain prone to factual inaccuracies and computational errors, including hallucinations and mistakes in mathematical reasoning. Recent work augmented LLMs with tools to mitigate these shortcomings, but often requires curated gold tool-use demonstrations. In this paper, we investigate whether LLMs can learn to use tools without demonstrations. First, we analyse zero-shot prompting strategies to guide LLMs in tool utilisation. Second, we propose a self-training method to synthesise tool-use traces using the LLM itself. We compare supervised fine-tuning and preference fine-tuning techniques for fine-tuning the model on datasets constructed using existing Question Answering (QA) datasets, i.e., TriviaQA and GSM8K. Experiments show that tool-use enhances performance on a long-tail knowledge task: 3.7% on PopQA, which is used solely for evaluation, but leads to mixed results on other datasets, i.e., TriviaQA, GSM8K, and NQ-Open. Our findings highlight the potential and challenges of integrating external tools into LLMs without demonstrations.</p> </div> <div class="bibtex hidden" style="font-size: 0.8em;"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">luo-etal-2024-large</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-Training Large Language Models for Tool-Use Without Demonstrations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luo, Ne and Gema, Aryo Pradipta and He, Xuanli and van Krieken, Emile and Lesci, Pietro and Minervini, Pasquale}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: NAACL 2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR 2025</abbr> </div> <div id="vanderwal-etal-2024-polypythias" class="col-sm-8"> <div class="title">PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs</div> <div class="author"> Oskar Van der Wal<sup>*</sup>, <em>Pietro Lesci<sup>*</sup></em>,  Max Müller-Eberstein, Naomi Saphra, Hailey Schoelkopf, Willem Zuidema, and Stella Biderman </div> <div class="periodical"> <em>In The Thirteenth International Conference on Learning Representations</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2503.09543" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="arxiv-logo" src="/assets/img/arxiv-logo.svg" alt="arXiv" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/arxiv-logo-light.svg' : '/assets/img/arxiv-logo.svg'"> </a> <a href="https://github.com/EleutherAI/pythia" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="github-logo" src="/assets/img/github-mark.svg" alt="GitHub" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/github-mark-light.svg' : '/assets/img/github-mark.svg'"> </a> <a href="https://huggingface.co/collections/EleutherAI/polypythias-67bed6916110c8933e1ea561" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/huggingface_logo-noborder.svg" alt="GitHub" style="height: 14px; vertical-align: middle;"> </a> </div> <div class="abstract hidden"> <p>The stability of language model pre-training and its effects on downstream performance are still understudied. Prior work shows that the training process can yield significantly different results in response to slight variations in initial conditions, e.g., the random seed. Crucially, the research community still lacks sufficient resources and tools to systematically investigate pre-training stability, particularly for decoder-only language models. We introduce the PolyPythias, a set of 45 new training runs for the Pythia model suite: 9 new seeds across 5 model sizes, from 14M to 410M parameters, resulting in about 7k new checkpoints that we release. Using these new 45 training runs, in addition to the 5 already available, we study the effects of different initial conditions determined by the seed – i.e., parameters’ initialisation and data order – on (i) downstream performance, (ii) learned linguistic representations, and (iii) emergence of training phases. In addition to common scaling behaviours, our analyses generally reveal highly consistent training dynamics across both model sizes and initial conditions. Further, the new seeds for each model allow us to identify outlier training runs and delineate their characteristics. Our findings show the potential of using these methods to predict training stability.</p> </div> <div class="bibtex hidden" style="font-size: 0.8em;"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vanderwal-etal-2024-polypythias</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{PolyPythias}: {S}tability and Outliers across Fifty Language Model Pre-Training Runs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{{Van der Wal}, Oskar and Lesci, Pietro and and Max M\"{u}ller-Eberstein and Saphra, Naomi and Schoelkopf, Hailey and Zuidema, Willem and Biderman, Stella}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Thirteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=bmrYu2Ekdz}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP 2024</abbr> </div> <div id="diehl-martinez-etal-2024-tending" class="col-sm-8"> <div class="title">Tending Towards Stability: Convergence Challenges in Small Language Models</div> <div class="author"> Richard Diehl Martinez, <em>Pietro Lesci</em>, and Paula Buttery </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: EMNLP 2024</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2410.11451" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="arxiv-logo" src="/assets/img/arxiv-logo.svg" alt="arXiv" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/arxiv-logo-light.svg' : '/assets/img/arxiv-logo.svg'"> </a> <a href="https://github.com/rdiehlmartinez/pretraining-playground" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="github-logo" src="/assets/img/github-mark.svg" alt="GitHub" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/github-mark-light.svg' : '/assets/img/github-mark.svg'"> </a> <a href="https://huggingface.co/pretraining-playground" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/huggingface_logo-noborder.svg" alt="GitHub" style="height: 14px; vertical-align: middle;"> </a> </div> <div class="abstract hidden"> <p>Increasing the number of parameters in language models is a common strategy to enhance their performance. However, smaller language models remain valuable due to their lower operational costs. Despite their advantages, smaller models frequently underperform compared to their larger counterparts, even when provided with equivalent data and computational resources. Specifically, their performance tends to degrade in the late pretraining phase. This is anecdotally attributed to their reduced representational capacity. Yet, the exact causes of this performance degradation remain unclear. We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers’ activations to their parameters’ effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models.</p> </div> <div class="bibtex hidden" style="font-size: 0.8em;"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">diehl-martinez-etal-2024-tending</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tending Towards Stability: Convergence Challenges in Small Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Diehl Martinez, Richard and Lesci, Pietro and Buttery, Paula}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: EMNLP 2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Miami, Florida, USA}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3275--3286}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.findings-emnlp.187}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL 2024</abbr> <div class="d-flex justify-content-center"> <span class="myaward">Best Paper Award</span> </div> </div> <div id="lesci-etal-2024-causal" class="col-sm-8"> <div class="title">Causal Estimation of Memorisation Profiles</div> <div class="author"> <em>Pietro Lesci</em>, Clara Meister, Thomas Hofmann, Andreas Vlachos, and Tiago Pimentel </div> <div class="periodical"> <em>In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2406.04327" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="arxiv-logo" src="/assets/img/arxiv-logo.svg" alt="arXiv" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/arxiv-logo-light.svg' : '/assets/img/arxiv-logo.svg'"> </a> <a href="https://github.com/pietrolesci/memorisation-profiles" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="github-logo" src="/assets/img/github-mark.svg" alt="GitHub" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/github-mark-light.svg' : '/assets/img/github-mark.svg'"> </a> <a href="https://huggingface.co/collections/pietrolesci/memorisation-profiles-6619604c4594c878cd9d451f" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/huggingface_logo-noborder.svg" alt="GitHub" style="height: 14px; vertical-align: middle;"> </a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Award</p> </div> <div class="abstract hidden"> <p>Understanding memorisation in language models has practical and societal implications, e.g., studying models’ training dynamics or preventing copyright infringements.Prior work defines memorisation as the causal effect of training with an instance on the model’s ability to predict that instance. This definition relies on a counterfactual: the ability to observe what would have happened had the model not seen that instance.Existing methods struggle to provide computationally efficient and accurate estimates of this counterfactual. Further, they often estimate memorisation for a model architecture rather than for a specific model instance. This paper fills an important gap in the literature, proposing a new, principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics. Using this method, we characterise a model’s memorisation profile–its memorisation trends across training–by only observing its behaviour on a small set of instances throughout training.In experiments with the Pythia model suite, we find that memorisation (i) is stronger and more persistent in larger models, (ii) is determined by data order and learning rate, and (iii) has stable trends across model sizes, thus making memorisation in larger models predictable from smaller ones.</p> </div> <div class="bibtex hidden" style="font-size: 0.8em;"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lesci-etal-2024-causal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Causal Estimation of Memorisation Profiles}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lesci, Pietro and Meister, Clara and Hofmann, Thomas and Vlachos, Andreas and Pimentel, Tiago}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Bangkok, Thailand}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{15616--15635}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.acl-long.834}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.acl-long.834}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NAACL 2024</abbr> </div> <div id="lesci-vlachos-2024-anchoral" class="col-sm-8"> <div class="title">AnchorAL: Computationally Efficient Active Learning for Large and Imbalanced Datasets</div> <div class="author"> <em>Pietro Lesci</em>, and Andreas Vlachos </div> <div class="periodical"> <em>In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2404.05623" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="arxiv-logo" src="/assets/img/arxiv-logo.svg" alt="arXiv" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/arxiv-logo-light.svg' : '/assets/img/arxiv-logo.svg'"> </a> <a href="https://github.com/pietrolesci/anchoral" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="github-logo" src="/assets/img/github-mark.svg" alt="GitHub" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/github-mark-light.svg' : '/assets/img/github-mark.svg'"> </a> <a href="https://huggingface.co/collections/pietrolesci/anchoral-66103ace42da659656c635d2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/huggingface_logo-noborder.svg" alt="GitHub" style="height: 14px; vertical-align: middle;"> </a> </div> <div class="abstract hidden"> <p>Active learning for imbalanced classification tasks is challenging as the minority classes naturally occur rarely. Gathering a large pool of unlabelled data is thus essential to capture minority instances. Standard pool-based active learning is computationally expensive on large pools and often reaches low accuracy by overfitting the initial decision boundary, thus failing to explore the input space and find minority instances. To address these issues we propose AnchorAL. At each iteration, AnchorAL chooses class-specific instances from the labelled set, or anchors, and retrieves the most similar unlabelled instances from the pool. This resulting subpool is then used for active learning. Using a small, fixed-sized subpool AnchorAL allows scaling any active learning strategy to large pools. By dynamically selecting different anchors at each iteration it promotes class balance and prevents overfitting the initial decision boundary, thus promoting the discovery of new clusters of minority instances. Experiments across different classification tasks, active learning strategies, and model architectures AnchorAL is (i) faster, often reducing runtime from hours to minutes, (ii) trains more performant models, (iii) and returns more balanced datasets than competing methods.</p> </div> <div class="bibtex hidden" style="font-size: 0.8em;"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lesci-vlachos-2024-anchoral</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{AnchorAL}: {C}omputationally Efficient Active Learning for Large and Imbalanced Datasets}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lesci, Pietro and Vlachos, Andreas}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Mexico City, Mexico}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8445--8464}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.48550/arXiv.2404.05623}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.naacl-long.467}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Duh, Kevin and Gomez, Helena and Bethard, Steven}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL 2023</abbr> </div> <div id="lesci-etal-2023-diable" class="col-sm-8"> <div class="title">Diable: Efficient Dialogue State Tracking as Operations on Tables</div> <div class="author"> <em>Pietro Lesci</em>, Yoshinari Fujinuma, Momchil Hardalov, Chao Shang, Yassine Benajiba, and Lluis Marquez </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: ACL 2023</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2305.17020" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="arxiv-logo" src="/assets/img/arxiv-logo.svg" alt="arXiv" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/arxiv-logo-light.svg' : '/assets/img/arxiv-logo.svg'"> </a> <a href="https://github.com/amazon-science/efficient-dialogue-state-tracking-by-sequential-information-processing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img id="github-logo" src="/assets/img/github-mark.svg" alt="GitHub" style="height: 14px; vertical-align: middle;" onload="this.src = document.documentElement.getAttribute('data-theme') === 'dark' ? '/assets/img/github-mark-light.svg' : '/assets/img/github-mark.svg'"> </a> <a href="https://huggingface.co/collections/pietrolesci/dialogue-state-tracking-datasets-673388206612435d0e09694f" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/huggingface_logo-noborder.svg" alt="GitHub" style="height: 14px; vertical-align: middle;"> </a> </div> <div class="abstract hidden"> <p>Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to noisy data annotations due to the table operations approach.</p> </div> <div class="bibtex hidden" style="font-size: 0.8em;"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lesci-etal-2023-diable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Diable: Efficient Dialogue State Tracking as Operations on Tables}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lesci, Pietro and Fujinuma, Yoshinari and Hardalov, Momchil and Shang, Chao and Benajiba, Yassine and Marquez, Lluis}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: ACL 2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9697--9719}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.findings-acl.615}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.findings-acl.615}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Pietro Lesci. Last updated: June 23, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>